# =============================================================================
# Homelab Proxmox Environment Configuration
# =============================================================================
# Copy this file to .envrc and fill in your values
# Run: direnv allow
#
# IMPORTANT: Never commit .envrc to git! It contains sensitive information.
# =============================================================================

# -----------------------------------------------------------------------------
# Cloudflare API Configuration (for cert-manager DNS-01 challenges)
# -----------------------------------------------------------------------------
# API Token created with: Zone > DNS > Edit permissions for codeofficer.com
# Used by cert-manager to manage DNS records for Let's Encrypt certificates
export CLOUDFLARE_API_TOKEN="your-cloudflare-api-token-here"
export CLOUDFLARE_ZONE_ID="your-zone-id-here"
export CLOUDFLARE_ACCOUNT_ID="your-account-id-here"
export CLOUDFLARE_EMAIL="your-cloudflare-email@example.com"

# -----------------------------------------------------------------------------
# Proxmox API Configuration
# -----------------------------------------------------------------------------
# Create API token in Proxmox:
# 1. Go to Datacenter > Permissions > API Tokens
# 2. Create token for user (e.g., root@pam!terraform)
# 3. Set privileges: PVEDatastoreUser, PVEVMAdmin, PVEPoolAdmin
export PROXMOX_NODE_IP="10.20.11.11"  # First Proxmox node (pve-01)
export TF_VAR_proxmox_api_url="https://${PROXMOX_NODE_IP}:8006/api2/json"
export TF_VAR_proxmox_api_token_id="root@pam!terraform"
export TF_VAR_proxmox_api_token_secret="xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
export TF_VAR_proxmox_insecure="true"  # Set to "false" with valid SSL certs
export TF_VAR_proxmox_ssh_user="root"

# -----------------------------------------------------------------------------
# Packer Configuration (for VM template creation)
# -----------------------------------------------------------------------------
# Uses same Proxmox credentials as Terraform
# Create separate API token if desired: root@pam!packer
export PKR_VAR_proxmox_api_url="${TF_VAR_proxmox_api_url}"
export PKR_VAR_proxmox_api_token_id="${TF_VAR_proxmox_api_token_id}"
export PKR_VAR_proxmox_api_token_secret="${TF_VAR_proxmox_api_token_secret}"
export PKR_VAR_proxmox_node="pve-01"
export PKR_VAR_template_name="ubuntu-2404-k3s-template"
export PKR_VAR_vm_id="9000"
export PKR_VAR_ssh_password="ubuntu"  # Temporary password for template build
export PKR_VAR_storage_pool="local-zfs"
export PKR_VAR_iso_storage_pool="local"

# -----------------------------------------------------------------------------
# K3s Cluster Configuration
# -----------------------------------------------------------------------------
export TF_VAR_k3s_cluster_name="homelab"
export TF_VAR_k3s_version="v1.33.6+k3s1"
export TF_VAR_k3s_cluster_domain="k3s.home.arpa"

# -----------------------------------------------------------------------------
# VM Template Configuration
# -----------------------------------------------------------------------------
# VM template created by Packer (make template)
# Must match PKR_VAR_template_name and PKR_VAR_vm_id above
export TF_VAR_vm_template_name="ubuntu-2404-k3s-template"
export TF_VAR_vm_template_vmid="9000"
export TF_VAR_vm_ssh_user="ubuntu"
export TF_VAR_vm_ssh_password="your-secure-password-here"

# SSH public key for VM access (REQUIRED)
# Get your public key: cat ~/.ssh/id_ed25519.pub
export TF_VAR_vm_ssh_keys='["ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIDI6acDm78Cj0ntqjVqYsNTbJNvB/nUOyvk9JBnDXfhv codeofficer@gmail.com"]'

# -----------------------------------------------------------------------------
# Network Configuration
# -----------------------------------------------------------------------------
export TF_VAR_dns_servers='["10.20.11.1"]'  # UDM Pro as DNS
export TF_VAR_search_domain="home.arpa"

# -----------------------------------------------------------------------------
# Storage Configuration
# -----------------------------------------------------------------------------
# Available storage pools:
#   - local-zfs: Fast local NVMe ZFS (~950GB) - for VM disks, templates
#   - nas-vmstorage: NFS from UNAS Pro (~37TB) - for backups, shared storage
export TF_VAR_storage_pool="local-zfs"

# -----------------------------------------------------------------------------
# K3s Server Nodes (Control Plane)
# -----------------------------------------------------------------------------
# 2 control plane nodes on pve-02 and pve-03 (VLAN 11)
export TF_VAR_k3s_server_nodes='[
  {
    "name": "k3s-cp-01",
    "target_node": "pve-02",
    "vmid": 200,
    "ip_address": "10.20.11.80",
    "gateway": "10.20.11.1",
    "cpu_cores": 4,
    "memory": 8192,
    "disk_size": "50G"
  },
  {
    "name": "k3s-cp-02",
    "target_node": "pve-03",
    "vmid": 201,
    "ip_address": "10.20.11.81",
    "gateway": "10.20.11.1",
    "cpu_cores": 4,
    "memory": 8192,
    "disk_size": "50G"
  }
]'

# -----------------------------------------------------------------------------
# K3s Agent Nodes (Workers)
# -----------------------------------------------------------------------------
# 1 GPU worker node on pve-01 (VLAN 11)
export TF_VAR_k3s_agent_nodes='[
  {
    "name": "k3s-gpu-01",
    "target_node": "pve-01",
    "vmid": 210,
    "ip_address": "10.20.11.85",
    "gateway": "10.20.11.1",
    "cpu_cores": 8,
    "memory": 16384,
    "disk_size": "100G",
    "gpu_passthrough": true
  }
]'

# -----------------------------------------------------------------------------
# GPU Passthrough Configuration
# -----------------------------------------------------------------------------
# GPU device mapping name configured in Proxmox
# Create mapping: Datacenter > Resource Mappings > PCI Devices
# Must match the name created in Proxmox (e.g., "rtx4000ada")
export TF_VAR_gpu_mapping_name="rtx4000ada"

# -----------------------------------------------------------------------------
# Application Secrets (for Kubernetes deployments)
# -----------------------------------------------------------------------------

# PostgreSQL
export POSTGRESQL_PASSWORD="change-me-strong-password"

# Redis
export REDIS_PASSWORD="change-me-strong-password"

# MinIO (S3-compatible object storage)
export MINIO_ROOT_USER="admin"
export MINIO_ROOT_PASSWORD="change-me-strong-password"

# n8n (Workflow Automation)
export N8N_ENCRYPTION_KEY="change-me-32-char-encryption-key"
export N8N_DB_PASSWORD="change-me-strong-password"

# Home Assistant
export HOMEASSISTANT_PASSWORD="change-me-strong-password"

# Grafana
export GRAFANA_ADMIN_PASSWORD="change-me-strong-password"

# -----------------------------------------------------------------------------
# Kubectl Configuration
# -----------------------------------------------------------------------------
# Path to kubeconfig for cluster access
export KUBECONFIG="${PWD}/infrastructure/terraform/kubeconfig"

# -----------------------------------------------------------------------------
# Utility Exports
# -----------------------------------------------------------------------------
# First server IP for direct SSH access
export FIRST_SERVER_IP="10.20.11.80"

# First agent IP for testing
export FIRST_AGENT_IP="10.20.11.85"

# =============================================================================
# End of Configuration
# =============================================================================
