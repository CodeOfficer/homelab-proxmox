# kube-prometheus-stack Helm Chart values
# Prometheus + Grafana + Alertmanager monitoring stack
# 15-day retention, local-path storage, control plane scheduling

# Prometheus server configuration
prometheus:
  prometheusSpec:
    retention: 15d
    retentionSize: "8GB"

    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: local-path
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 10Gi

    resources:
      requests:
        memory: 400Mi
        cpu: 200m
      limits:
        memory: 2Gi
        cpu: 1000m

    # Run on control plane nodes
    nodeSelector:
      node-role.kubernetes.io/control-plane: "true"
    tolerations:
      - key: "node-role.kubernetes.io/control-plane"
        operator: "Exists"
        effect: "NoSchedule"

# Grafana configuration
grafana:
  # Admin password set via --set in deploy.sh
  adminUser: admin

  # Install additional plugins
  plugins:
    - marcusolsson-treemap-panel
    - knightss27-weathermap-panel

  # Disable init chown (causes issues with existing PVCs)
  initChownData:
    enabled: false

  persistence:
    enabled: true
    storageClassName: local-path
    size: 2Gi

  resources:
    requests:
      memory: 128Mi
      cpu: 100m
    limits:
      memory: 512Mi
      cpu: 500m

  # Run on control plane nodes
  nodeSelector:
    node-role.kubernetes.io/control-plane: "true"
  tolerations:
    - key: "node-role.kubernetes.io/control-plane"
      operator: "Exists"
      effect: "NoSchedule"

  # Ingress handled separately via ingress.yaml
  ingress:
    enabled: false

  # Default dashboards
  defaultDashboardsEnabled: true
  defaultDashboardsTimezone: America/Los_Angeles

# Alertmanager configuration
alertmanager:
  # Alert routing config (Telegram bot token/chat_id injected via deploy.sh --set)
  config:
    global:
      resolve_timeout: 5m
    receivers:
      - name: "null"  # Sink for warnings (visible in Grafana only)
      - name: telegram-critical
        telegram_configs:
          - bot_token: "PLACEHOLDER"  # Injected via --set
            chat_id: "PLACEHOLDER"    # Injected via --set
            parse_mode: HTML
            message: |
              {{ if eq .Status "firing" }}ðŸš¨{{ else }}âœ…{{ end }} <b>{{ .Status | toUpper }}</b>
              <b>Alert:</b> {{ .CommonLabels.alertname }}
              <b>Severity:</b> {{ .CommonLabels.severity }}
              {{ range .Alerts }}
              {{ .Annotations.description }}
              {{ end }}
    route:
      receiver: "null"
      group_by: ['alertname', 'namespace']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 4h
      routes:
        - match:
            severity: critical
          receiver: telegram-critical

  alertmanagerSpec:
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: local-path
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 1Gi

    resources:
      requests:
        memory: 64Mi
        cpu: 50m
      limits:
        memory: 256Mi
        cpu: 200m

    nodeSelector:
      node-role.kubernetes.io/control-plane: "true"
    tolerations:
      - key: "node-role.kubernetes.io/control-plane"
        operator: "Exists"
        effect: "NoSchedule"

# Node exporter - runs on all nodes
nodeExporter:
  enabled: true

# kube-state-metrics
kubeStateMetrics:
  enabled: true

# Disable components we don't need
pushgateway:
  enabled: false

# Prometheus Operator
prometheusOperator:
  resources:
    requests:
      memory: 128Mi
      cpu: 100m
    limits:
      memory: 512Mi
      cpu: 500m

  nodeSelector:
    node-role.kubernetes.io/control-plane: "true"
  tolerations:
    - key: "node-role.kubernetes.io/control-plane"
      operator: "Exists"
      effect: "NoSchedule"
